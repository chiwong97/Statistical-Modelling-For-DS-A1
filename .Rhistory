summary(train.glm.fit)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
train = sample(10000,7000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
summary(train.glm.fit)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
train = sample(10000,8000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
summary(train.glm.fit)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part d:
train = sample(10000,5000)
train.glm.fit = glm(default ~ balance + income + student, data = Default, family = "binomial", subset = train)
summary(train.glm.fit)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part b: Estimating the test error of the model
train = sample(10000,5000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part b: Estimating the test error of the model
train = sample(10000,5000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
set.seed(20)
# Part b: Estimating the test error of the model
train = sample(10000,5000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
train = sample(dim(Default)[1], dim(Default)[1] / 2)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
train = sample(dim(Default)[1], dim(Default)[1] / 2)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part b: Estimating the test error of the model
train = sample(10000,5000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part c: Repeat 3 different times with 3 different splits of training and validation sets
train = sample(10000,6000)
train.glm.fit = glm(default ~ balance + income, data = Default, family = "binomial", subset = train)
train.glm.prob = predict(train.glm.fit, newdata = Default[-train, ], type = "response")
predict.glm = rep("No", length(train.glm.prob))
predict.glm[train.glm.prob > 0.5] = "Yes"
mean(predict.glm != Default[-train, ]$default)
# Part a
attach(Auto)
set.seed(20)
mpg01 = rep(0, nrow(Auto))
mpg01[mpg > median(mpg)] = 1
newAuto = data.frame(Auto, mpg01)
str(newAuto)
# Part b: mpg01 against other features graphically (scatterplot and boxplots)
cor(newAuto[,-9])
pairs(newAuto)
# Part c: splitting into training and test
train = sample(1:nrow(newAuto), .5*nrow(newAuto), replace=FALSE)
newAuto.train = Auto[train.OBS,]
newAuto.test = Auto[-train.OBS,]
# Part c: splitting into training and test
train = sample(1:nrow(newAuto), .5*nrow(newAuto), replace=FALSE)
newAuto.train = Auto[train, ]
newAuto.test = Auto[-train, ]
mpg01.test = Auto.test$mpg01
mpg01.test = newAuto.test$mpg01
# Part c: splitting into training and test
train_ind = sample(1:nrow(newAuto), .5*nrow(newAuto), replace=FALSE)
train = newAuto[train, ]
test = newAuto[-train, ]
test = newAuto[-train, ]
newAuto = data.frame(Auto, mpg01)
train = newAuto[train, ]
test = newAuto[-train, ]
# Part c: splitting into training and test
train_ind = sample(1:nrow(newAuto), .5*nrow(newAuto), replace=FALSE)
train = newAuto[train, ]
# Part a
attach(Auto)
set.seed(20)
mpg01 = rep(0, nrow(Auto))
mpg01[mpg > median(mpg)] = 1
newAuto = data.frame(Auto, mpg01)
attach(newAuto)
# Part b: mpg01 against other features graphically (scatterplot and boxplots)
cor(newAuto[,-9])
pairs(newAuto)
# Part c: splitting into training and test
train_ind = sample(1:nrow(newAuto), .5*nrow(newAuto), replace=FALSE)
train = newAuto[train, ]
test = newAuto[-train, ]
mpg01.test = test$mpg01
train.OBS=sample(1:nrow(Auto),.5*nrow(Auto),replace=FALSE)
Auto.train = Auto[train.OBS,]
Auto.test = Auto[-train.OBS,]
mpg01.test = Auto.test$mpg01
# Part a
attach(Auto)
set.seed(20)
mpg01 = rep(0, nrow(Auto))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
# Part b: mpg01 against other features graphically (scatterplot and boxplots)
cor(Auto[,-9])
pairs(Auto)
# Part c: splitting into training and test
train_ind = sample(1:nrow(Auto), .5*nrow(Auto), replace=FALSE)
train = Auto[train, ]
test = Auto[-train, ]
mpg01.test = test$mpg01
# Part c: splitting into training and test
train_ind = sample(1:nrow(Auto),.5*nrow(Auto), replace=FALSE)
train = Auto[train, ]
train.OBS=sample(1:nrow(Auto),.5*nrow(Auto),replace=FALSE)
Auto.train = Auto[train.OBS,]
Auto.test = Auto[-train.OBS,]
mpg01.test = Auto.test$mpg01
# Part c: splitting into training and test
train_ind = sample(1:nrow(Auto),.5*nrow(Auto), replace=FALSE)
train = Auto[train_ind, ]
test = Auto[-train_ind, ]
mpg01.test = test$mpg01
lda.fit = lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, subset = train)
library(MASS)
lda.fit = lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, subset = train)
lda.fit = lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, subset = unlist(train))
lda.fit = lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, subset = unlist(train))
lda.pred = predict(lda.fit, test)
lda.class = lda.pred$class
mean(lda.class != mpg01.test)
# Part e: QDA
qda.fit = qda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, subset = unlist(train))
qda.pred = predict(qda.fit, test)
qda.class = qda.pred$class
mean(qda.class != mpg01.test)
# Part f: Logistic Regression
glm.fit = glm(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto, family = "binomial", subset = unlist(train))
glm.probs = predict(glm.fit, test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
glm.error = mean(glm.pred != mpg01.test)
glm.error
train.X = cbind(cylinders, displacement, horsepower, weight)[train_ind, ]
test.X = cbind(cylinders, displacement, horsepower, weight)[-train_ind, ]
# Part g: KNN
library(class)
train.X = cbind(cylinders, displacement, horsepower, weight)[train_ind, ]
test.X = cbind(cylinders, displacement, horsepower, weight)[-train_ind, ]
mpg01.train = train$mpg01
knn.pred = knn(train.X, test.X, mpg01.train, k = 1)
knn.error.kequals1 = mean(knn.pred != mpg01.test)
knn.error.kequals1
knn.pred = knn(train.X, test.X, mpg01.train, k = 5)
knn.error.kequals5 = mean(knn.pred != mpg01.test)
knn.error.kequals5
knn.pred = knn(train.X, test.X, mpg01.train, k = 10)
knn.error.kequals10 = mean(knn.pred != mpg01.test)
knn.error.kequals10
knn.pred = knn(train.X, test.X, mpg01.train, k = 20)
knn.error.kequals20 = mean(knn.pred != mpg01.test)
knn.error.kequals20
# Question 1
?contr.helmert
contr.helmert(4)
contr.helmert(4)
# Computatioal Stats
# Assignment 2
# Question 1
# a
set.seed(20)
n = 100
X = rnorm(n)
e = rnorm(n)
# b
b0 = 6
# b
b0 = 6
b1 = 8
b2 = 10
b3 = 12
Y = b0 + b1*X + b2*X^2 + b3*X^4 + e
# c
library(leaps)
data = data.frame(Y, X)
model = regsubsets(Y ~ X + X^2 + X^3 + X^4 + X^5 + X^6 + X^7 + X^8 + X^9 + X^10, data = data, nvmax = 10)
?I
model = regsubsets(Y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10), data = data, nvmax = 10)
summary(model)
# plot 1
plot(model$cp, type = "l", col = 4, xlab = "Number of Variables", ylab = "Cp")
?plot
# plot 1
plot(model$cp, type = "l", xlab = "Number of Variables", ylab = "Cp")
# plot 1
plot(model$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
?poly
# c
library(leaps)
newdataset = data.frame(Y, X)
model = regsubsets(Y ~ poly(X, 10), data = newdataset, nvmax = 10)
summary(model)
# plot 1
plot(model$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
model$cp
# Computational Stats
# Assignment 2
# Question 2
# a
n = 100
X1 = rnorm(n, 1, 2)
X2 = rgamma(n, 1)
e = rnorm(n, 0 ,1)
# Computational Stats
# Assignment 2
# Question 2
# a
set.seed(20)
n = 100
X1 = rnorm(n)
X2 = rnorm(n)
e = rnorm(n)
# b
b1 = 10
Y = rnorm(n)
# b
b1 = 10
# c
a = Y - b1 * X1
b2 = lm(a ~ X2)$coef[2]
# e
# initailizing the coefficients
b0 = NULL
b1 = 10
b2 = NULL
a = NULL
iterations = 1000
for (i in 1:iterations) {
a = Y - b1[i] * X1
b2[i] = lm(a ~ X2)$coef[2]
a = Y - b2[i] * X2
b0[i] = lm(a ~ X1)$coef[1]
b1[i+1] = lm(a ~ X1)$coef[2]
}
betas <- data.frame(b0, b1[2:1001], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
betas <- data.frame(b0, b1[1:iterations], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
betas <- data.frame(b0, b1, b2)
betas <- data.frame(b0, b1[1:iterations], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
plot(betas$b0, col = "red", type = "l", xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?plot
plot(betas$b0, col = "red", type = "l", xlim = c(1,50), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?colnames
# f
mreg = lm(Y ~ X1 + X2)
summary(mreg)
mreg$coefficients[1]
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?abline
abline(h = mreg@coefficients[1])
abline(h = mreg@coefficients[1], col = "black")
abline(h = mreg$coefficients[1], col = "black")
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
# f
mreg = lm(Y ~ X1 + X2)
mreg$coefficients[1]
?abline
abline(h = mreg$coefficients[1], col = "black", lty = 2)
abline(h = mreg$coefficients[2], col = "black", lty = 2)
abline(h = mreg$coefficients[3], col = "black", lty = 2)
# h
# initializing the coefficients
?rep
# Initial guess for the parameter b2,b3,...,b100
?rep
?head
# h
# Generating the simple linear regression with 100 parameters and 1000 observations
set.seed(50)
betas = rnorm(100)
b0 = 10
X = matrix(rnorm(1000 * 100), 1000, 100)
e = rnorm(1000)
Y = b0 + X%*%betas + e
# generating initial values for the parameter b2,b3,...,b100
initialBetaEstimates = rep(NA, 100)
b0Initial = NA
initialBetaEstimates[2:100] = rnorm(99)
# generating a matrix to hold the estimates made in 100 iterations of backfitting
iterations = 100
bmatrix = matrix(nrow = iterations + 1, ncol = 100 + 1)
bmatrix[1, 3:101] = initialBetaEstimates[2:100]
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p]$coef[2])
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(boInitial, initialBetaEstimates)
}
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p])$coef[2]
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(boInitial, initialBetaEstimates)
}
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p])$coef[2]
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(b0Initial, initialBetaEstimates)
}
colnames(bmatrix) = c(paste0("b", 0:100))
# reporting the estimates of the first 20 iterations
head(bmatrix, 20)
# reporting the estimates of the first 20 iterations
head(bmatrix, 20)
# Multiple linear regression
mreg = lm(Y ~ X)
mreg$coefficients
# reporting the estimates of the first 10 iterations
head(bmatrix, 20)
# reporting the estimates of the first 10 iterations
head(bmatrix, 10)
# Multiple linear regression
mreg = lm(Y ~ X)
mreg$coefficients
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[0], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# reporting the estimates of the first 10 iterations
head(bmatrix, 10)
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
lines(bmatrix[,2], col = "blue")
lines(bmatrix[,3], col = "green")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), ylim = c(-2, 11), xlab = "Number of Iterations", ylab = "Beta Estimate")
lines(bmatrix[,2], col = "blue")
lines(bmatrix[,3], col = "green")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,2], col = "red", type = "l", xlim = c(1,20), ylim = c(-2, 2), xlab = "Number of Iterations", ylab = "Beta Estimate")
plot(bmatrix[,2], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
# plot for b0
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# plot for b2,...,b9
plot(bmatrix[,3], col = "green", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimates")
lines(bmatrix[,4], col = "purple")
# plot for b3
plot(bmatrix[,4], col = "purple", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimates")
# Plotting only b0, b1, b2 and b3
# plot for b0
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
abline(mreg$coefficients[1], col = "black", lty = 2)
abline(h = mreg$coefficients[1], col = "black", lty = 2)
# Assignment 2
# Computational Stats
# Question 3
# a
library(ISLR)
data(OJ)
set.seed(20)
data(OJ)
train = sample(nrow(OJ), 800)
OJtrain = OJ[train, ]
OJtest = OJ[-train, ]
tree = tree(Purchase ~ ., data = OJtrain)
# b
library(tree)
# b
install.packages(tree)
# b
install.packages("tree")
?floor
?%%
10%%3
floor(10)
10.1%%3
floor(10/3)
floor(13/2)
ceiling(13/2)
abs(6.5)
round(6.5)
round(6.49)
round(7.5)
round(7.51)
round(6.51)
A = matrix(c(1,2,3,4,5,6), 2, 3, byrow = TRUE)
A
a = c(1:8,23,10,1:7)
a
?rep
rep(2,6)
rep(c(2,3,4), 3)
rep(c(1,2,3,4,5), 1:5)
contr.sum(4)
contr.sum(5)
contr.sum(3)
?contr.sum
contr.sum(4)
contr.treatment(4)
data(iris)
colnames(iris)
plot(iris[,1:4])
plot(iris[,1:4],col=c(2,3,4)[iris$Species],main='Iris data')
# multivariate stats week 4 & 5 lab
?iris
EVV = eigen(cov(iris[,1:4]))
EVV
vec = EVV$vectors
lambda = EVV$values
# PCA
n = nrow(iris)
X=as.matrix(iris[,1:4])
PCX=(X-matrix(rep(1,n),nrow=n)%*%colMeans(X))%*%vec
fracvar=lambda/sum(lambda)
fracvar
lambda
plot(PCX[,1],rep(0,n),col=iris$Species,xlab='PC1',ylab='',yaxt="n")
plot(PCX[,1],PCX[,2],col=iris$Species,xlab='PC1',ylab='PC2')
# lambda tells us the variance of each of the 4 PC's
lambda
# PCA using the R function
PCX = prcomp(iris[,1:4], retx = TRUE)
vec=PCX$rotation
lambda=PCX$sdev^2
Y=PCX$x
vec
lambda=PCX$sdev^2
lambda
Y
cumsum(lambda)/sum(lambda)
# eigenvectors and eigenvalues of the covariance matrix of the iris data
EVV = eigen(cov(iris[,1:4]))
EVV
# PCA using the R function
PCX = prcomp(iris[,1:4], retx = TRUE)
vec=PCX$rotation
# vec gives us the eigenvectors
vec
library(faraway)
data(fima)
data(pima)
help(pima)
View(pima)
#View(pima)
plot(pima)
summary(pima)
# Question 1 - create a factor version of the response variable
fit = glm(test ~ pregnant+glucose+diastolic+triceps+insulin+bmi+diabetes+age, data = pima, family = binomial)
summary(fit)
unlink(".RData")
source('~/Google Drive/Unimelb/Masters/Statistical Modelling for Data Science/Assignment 1/assignment1.R', echo=TRUE)
# smok.f:falc.f is insignificant, remove from model
model5 = glm(formula = dv.f ~ age + ms.f + mmo.f + smok.f + falc.f + educ + reg.f + ms.f:falc.f + mmo.f:alc.f, family = binomial, data = domviolence)
anova(model5, test = "Chi")
source('~/Google Drive/Unimelb/Masters/Statistical Modelling for Data Science/Assignment 1/assignment1.R', echo=TRUE)
